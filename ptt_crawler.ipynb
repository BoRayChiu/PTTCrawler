{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "mount_file_id": "1YYkQyGYP1Jbo4dckQUUqv2pZPil8uzWu",
      "authorship_tag": "ABX9TyMYEytGbrKmlcIpyfjHT9MR",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/BoRayChiu/ptt_crawler/blob/main/ptt_crawler.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "KneJ0lsmppjY",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e24a416b-a479-40ae-d3df-162c6702a263"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Board: gossiping\n",
            "Page: 1\n",
            "Finish!\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "import requests as rq\n",
        "from bs4 import BeautifulSoup as bsp\n",
        "import json\n",
        "import time\n",
        "import os\n",
        "\n",
        "url = \"https://www.ptt.cc\"\n",
        "headers = {\"cookie\": \"over18=1;\", \"User-Agent\": \"mozilla/5.0 (Linux; Android 6.0.1; \"\"Nexus 5x build/mtc19t applewebkit/537.36 (KHTML, like Gecko) \"\"Chrome/51.0.2702.81 Mobile Safari/537.36\"}\n",
        "def topic_crawler(board :str, frequency :int) ->list:\n",
        "  result_box = []\n",
        "  detailed_url = \"/bbs/\" + board + \"/index.html\"\n",
        "  for i in range(0, int(frequency)):\n",
        "    # request every demand page\n",
        "    result = rq.post(url + detailed_url, headers = headers)\n",
        "    result.encoding = \"utf-8\"\n",
        "    result_soup = bsp(result.text.strip(), \"html.parser\")\n",
        "    topic_anchor = result_soup.select_one(\".r-list-sep\")\n",
        "    if topic_anchor != None:\n",
        "      topics = topic_anchor.find_previous_siblings(class_ = \"r-ent\")\n",
        "    else:\n",
        "      topics = result_soup.select(\"r-cent\")\n",
        "    for element in topics:\n",
        "      # request every topic\n",
        "      title = element.select_one(\".title\")\n",
        "      title_url = title.select_one(\"a\")\n",
        "      meta = element.select_one(\".meta\")\n",
        "      if(title_url != None):\n",
        "        topic_url = url + title_url[\"href\"]\n",
        "      else:\n",
        "        continue\n",
        "      topic_result = rq.post(topic_url, headers = headers)\n",
        "      topic_result.encoding = \"utf-8\"\n",
        "      # bs4 topic\n",
        "      topic_result_soup = bsp(topic_result.text.strip(), \"html.parser\")\n",
        "      topic = topic_result_soup.select_one(\".bbs-screen.bbs-content\")\n",
        "      topic_meta = topic.select(\".article-metaline\")\n",
        "      meta_box = {}\n",
        "      topic_messages = topic.select(\".push\")\n",
        "      message_box = {}\n",
        "      # has meta information in topic page\n",
        "      if(topic_meta != []):\n",
        "        # meta infomation\n",
        "        for topic_meta_info in topic_meta:\n",
        "          meta_box[topic_meta_info.select_one(\".article-meta-tag\").text.strip()] = topic_meta_info.select_one(\".article-meta-value\").text.strip()\n",
        "        # contents\n",
        "        content = (topic.text.strip().split(\"--\")[0]).split(\"\\n\")[1:]\n",
        "        contents = \" \".join(content)\n",
        "        # messages\n",
        "        for message in topic_messages:\n",
        "          message_content = message.select_one(\".push-content\")\n",
        "          if message_content.select_one(\"a\") != None:\n",
        "            continue\n",
        "          message_userid = message.select_one(\".push-userid\").text.strip()\n",
        "          message_box[message_userid] = message_box.get(message_userid, \"\")  + \" \" + message_content.text.strip().strip(\":\").strip()\n",
        "        # intergrate\n",
        "        topic_box = {\n",
        "            \"meta_information\": meta_box,\n",
        "            \"contents\": contents,\n",
        "            \"messages\": message_box\n",
        "        }\n",
        "      else:\n",
        "        # meta information\n",
        "        meta_box[\"作者\"] = meta.select_one(\".author\").text.strip()\n",
        "        meta_box[\"標題\"] = title.text.strip()\n",
        "        meta_box[\"時間\"] = meta.select_one(\".date\").text.strip()\n",
        "        # contents\n",
        "        content = (topic.text.strip().split(\"--\")[0]).split(\"\\n\")[1:]\n",
        "        contents = \"\\n\".join(content)\n",
        "        # messages\n",
        "        for message in topic_messages:\n",
        "          message_content = message.select_one(\".push-content\")\n",
        "          if message_content.select_one(\"a\") != None:\n",
        "            continue\n",
        "          message_userid = message.select_one(\".push-userid\").text.strip()\n",
        "          message_box[message_userid] = message_box.get(message_userid, \"\")  + \" \" + message_content.text.strip().strip(\":\").strip()\n",
        "        # intergrate\n",
        "        topic_box = {\n",
        "            \"meta_information\": meta_box,\n",
        "            \"contents\": contents,\n",
        "            \"messages\": message_box\n",
        "        }\n",
        "      result_box.append(topic_box)\n",
        "    detailed_url = (result_soup.select(\".btn-group.btn-group-paging > a\")[1])[\"href\"]\n",
        "  return result_box\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "  storehouse = \"ptt_crawler_json_results\"\n",
        "\n",
        "  board = input(\"Board: \")\n",
        "  frequency = input(\"Page: \")\n",
        "  result = topic_crawler(board = board, frequency = frequency)\n",
        "  today = time.strftime(\"%Y-%m-%d\")\n",
        "  data = {\n",
        "    \"crawl_date\": today,\n",
        "    \"community\": \"PPT\",\n",
        "    \"crawl_results\": result\n",
        "  }\n",
        "  Root = os.path.join(\"drive/MyDrive\", storehouse)\n",
        "  if not os.path.exists(Root):\n",
        "    os.mkdir(Root)\n",
        "  with open(os.path.join(Root, today + \"-PPT-\" + board + \".json\"), \"w+\", encoding = \"utf-8\") as f:\n",
        "    json.dump(data, f, indent = 2, ensure_ascii = False)\n",
        "  print(\"Finish!\")"
      ]
    }
  ]
}